---
title: "Analyzing Compensation for Data Scientists"
author: "Anmol Srivastava, Juan Solorio, Matthew Rhodes, and Andres De La Fuente"
output: 
  pdf_document: 
classoption: twocolumn
#header-includes:
#  - \setlength{\parindent}{4em}
geometry: "left=2cm,right=2cm,top=2cm,bottom=2cm"

---

```{r setup, eval=TRUE,message=FALSE, warning=FALSE, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
library(ggplot2)
library(tidyverse)
library(ggpubr)
library(gridExtra)
library(grid)
library(scales)

```
```{r, echo =FALSE}
invisible(knitr::purl("../Code/01-Question1.Rmd", output="temp", quiet=TRUE))
```
```{r, echo =FALSE}
invisible(knitr::purl("../Code/03-Question3_AndyVersion.Rmd", output="temp3", quiet=TRUE))
```
```{r, echo =FALSE}
invisible(knitr::purl("../Code/02-Question2.Rmd", output="temp2", quiet=TRUE))
```

## Abstract
The goal of this project is to evaluate the effects of certain factors (such as job title and education) on observed differences in salary for working data scientists. The analysis employs techniques such as Welch t-tests, ANOVA, and regression, performed on data provided by Kaggle's 2017 "ML and DS Survey" (https://www.kaggle.com/kaggle/kaggle-survey-2017). The factors explored in this project are physical location (specifically, whether the respondent is in a 'high density' or 'low density' area), job title, recommended programming language, and education. We found evidence (insert p-value) supporting the notion that location determines differences in data scientists' salaries. We did not find sufficient evidence to claim that programming language determines such differences, even within individual job titles. However, we did find evidence for a significant difference in salaries between different job titles. A positive relationship betwen education level and salary was similarly supported by the data.


## Introduction

The focus of this project is compensation within the industry of data science. As prospective employees in the field, we have an interest in which factors might affect current data scientists' pay. Our analyses are built upon a dataset from Kaggle, which itself is the result of an industry-wide survey conducted on people working in the data science arena. This data is further explored in the 'Dataset Description' section. Then, after some exploratory analysis, we derived three questions upon which to center our research.

**Question 1**: Do data scientists' salaries differ between densely-populated and sparsely-populated areas?

**Question 2**: Do data scientists' salaries differ based on their job title? Do these salaries differ based on the programming languages recommended by respondents? 


**Question 3**: Do data scientists' salaries differ based on the level of education they have attained?

For each of these questions, we aimed to test for both the presence of significant differences between groups, and for more specific relationships (via regression).


## Dataset Description 

As previously mentioned, the dataset from which we drew our conclusions is Kaggle's 'ML and DS Survey' for 2017. Kaggle's subsequent results are summarized as follows: "For the first time, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. The survey received over 16,000 responses and we learned a ton about who is working with data, what’s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field." The survey asked an extremely broad scope of questions, which resulted in a mixture of multiple choice responses (e.g. "Education Level") and freeform responses (e.g. "Best description of undergraduate major?").

The dataset is structured as follows:

*schema.csv*: A .csv file with the survey schema. This schema includes the full, exact questions that correspond to each column name in both the multipleChoiceResponses.csv and freeformResponses.csv sheets.

*multipleChoiceResponses.csv*: Respondents' answers to multiple choice and ranking questions. These are non-randomized and thus a single row corresponds to all of a single user's answers.

*freeformResponses.csv*: Respondents' freeform answers to Kaggle's survey questions. These responses are randomized within a column, so that reading across a single row does not give a single user's answers.

*conversionRates.csv*: Currency conversion rates (to USD) as accessed from the R package "quantmod" on September 14, 2017.

*RespondentTypeREADME.txt*: This is a schema for decoding the contents of the schema.csv file. 

We used the values in the dataset's conversion rates file to generate compensation values in USD for all respondents. For the purposes of our analyses, we limited our focus to multipleChoiceReponses.csv.

The following is a simple histogram of our generated compensation data (in USD) from the survey, before any kind of manipulation.
```{r, echo =FALSE}
knitr::read_chunk("temp3")
```
```{r ref.label='Q3_data'}
```
```{r ref.label='Q3_overall_hist'}
```
The mean for this uncleaned data is `r mn` with a maximum value of `r mx`. Further exploration revealed that the dataset also contains a large quantity of zeros and values that seem too low to be someone's full-time pay. Based on our domain knowledge, we judged that these results were unrealistic, and therefore manipulated the data in a few important ways before applying our analyses.

First, we removed all entries with a zero for compensation, as this would either imply unemployment, entry error, or plain junk data, none of which were wanted for our analyses.

Second, we dichotomized compensation into two groups: 'salaries', and 'commissions' (the values which we decided are too low to be salaries). We decided to split the values at 20K; anything below this is considered commission.

Third, we decided that some values on the high end were either entry error or extreme outliers (there were values ranging from millions to billions of dollars), and should be capped. The capping value was set to $500,000 based on our real world experience. As evidenced below, the resulting dataset was much more reasonable to base further work on.
```{r, ref.label='Q3_capping'}
```
```{r ref.label='Q3_overall_hist'}
```

A secondary dataset was employed to help answer Question 1 (https://population.un.org/wpp/Download/Standard/Population/).
The dataset is described as follows:

Total annual population, by sex, from 1950 to 2100.

    PopMale: Total male population
    PopFemale: Total female population 
    PopTotal: Total population, both sexes 
    PopDensity: Population per square km
    
    (each of these is in thousands)


We were only interested in the lastest population, so we used the entries for 2019. After we had these values, they were converted in population per square mile for interpretability. The goal of this data was to allow for a determination of high-density and low-denisty countries. All of the countries that were included in the multipleChoiceReponses dataset were also included in the population dataset, so we appended the appropriate densities to their respective rows in the multipleChoiceReponses dataset as a final step before starting our analyses.


## Statistical Methods and Assumptions
**ANOVA:**

The first method we arrived at was Analysis of Variance (ANOVA).
From lecture, we know that ANOVA is designed to provide a single test of a null hypothesis of equal group means with a desired signiﬁcance level. It is a generalization of the equal-variance t-test to the case where the number of means to be compared is greater than 2. We are testing for difference in means of various groups, hence using ANOVA would be convenient and efficient.

**Regression**

The second test we thought to use was linear regression.
From lecture, we know that linear regression is equivalent to ANOVA when the variances are equal, and the null hypotheses for ANOVA and regression both imply that the mean response does not depend on the predictor. We also ackowledge that ANOVA and regression will not always agree, which is why we decided to run both.

**Welch T-Test**

After some exploratory analysis, we found that sample variances showed notable differences, meaning we might not necessarily satify the pertinent ANOVA assumptions for each group. This led us to conduct a Welch T-Test as well, addressing the differences in variances, and thus the issues that might arise from the use of ANOVA.

**Assumptions:**
For the each of the tests, we are working under the assumption that the samples included in the Kaggle Survey dataset are independent, meaning that no answers provided by one surveyee affected another surveyee's entry. Moreover, we are working under the assumption that we have sufficiently large data sizes for each group, to account for the normality requirement in both the ANOVA and Welch tests. For the ANOVA and regression models, we are also assuming equality in the variance of the groups. Finally, we presume normality and liniarity for the models, to construct either linear or generalized linear models.


## Pay & Location

Prior to conducting our analyses into Question 1, we suspected that the amount of information from the dense areas would be significantly larger than the data from sparse areas. We thought this because of the assumption that areas with a high population will have more of a need for data scientists than sparse areas (like rural countries). One important assumption we are making (which affected our choice of method) is that the data is normally distributed.
This means that areas that are on the lower end of sparse and dense will appear as often as areas that are on the higher end of dense and sparse, while the majority of areas fall closer to the mean of each population density. 
We initially felt that this would be a problem. However, even if the data wasn’t normally distributed, since we have around 10,000 rows and roughly 4 features, it is believed that the sample size is sufficiently large. Lastly, as previously stated, we make the equal-variance assumption.

```{r, echo =FALSE}
knitr::read_chunk("temp")
```

```{r ref.label='Q1_data'}
```

```{r ref.label='testing'}
```
Below are histograms for commission and salary, broken down into high density and low density areas.
```{r ref.label='pre_plots'}
```
From linear regression tests, there is a slightly negative linear relationship between density and compensation (for salary and commission).
The x-axis denotes 1 for high and 0 for low density, so we can see that (as density goes from low to high), average salary goes down by $21,652.
```{r ref.label='Question_1_plot_salary'}
```
This is the linear regression plot and the distribution of the total commission (including high and low density). Similarly, as density goes from low to high, average commission goes down by $2,106.
```{r ref.label='Question_1_plot_commission'}
```
```{r echo=FALSE}
unlink("temp")
```


## Pay & Job Title / Programming Language
```{r, echo =FALSE}
knitr::read_chunk("temp2")
```
```{r ref.label='Q2_setup'}
```
```{r ref.label='Q2_Jobtitlegroups'}
```
```{r ref.label='Q2_data'}
```
```{r ref.label='Q2_setup-dfs'}
```

For question 2, we wanted to explore the effects that a specific job title or preference in programming language might have on salary. For our purposes, our questions narrowed down to:

**Are there differences in data scientists' salaries when grouped by job-title (i.e. Data Analyst/Data Scientist, Engineer, Researcher), or by the type of programming language they employ most?**

Within the original survey, the participants were provided with 17 different titles as options for that which best suited their current role. To answer the first question we needed to create groups for the 17 different entries to the 'Current Job Title' descriptions. For this we decided upon grouping titles into 'Scientist' (people probably utilizing higher level math/statistics), 'ResearchAnalyst' (those focused with general data analysis and manipulation), 'Engineer' (titles focused on data engineering or software), and 'Other' (description unknown). We worked with the already-capped salary data for this question. 

Additionally, while there is no entry for "programming language participant most frequently employs", surveyees were given the free-hand option of 'Language Recommendation' they would give to people wanting to enter their field, which we used as a proxy. There were 14 languages (ex: Java, Python, R, Scale, etc), of which we were only interested in Python, R, and SQL (these were the top 3 recommendations). This narrowed our original dataset of over 16,000 rows, to one of around 3,000 for this part of the project, distributed as shown here:

```{r ref.label='Q2_job-den-bp-figures-general'}
```
```{r ref.label='Q2_lang-den-bp-figures-general'}
```
```{r}
hist.gen.jobs
hist.gen.lang
```


Working with this subset for job titles and programming languages, we had satistifed the need for a sufficiently large dataset, but we still needed to make sure our assumptions of equal variance and normality were held to then apply our tests. The data was passed into some box-plots and density plots to check for the variance and normality of the distribution as shown in the figure here:

```{r}
plot.jobs
plot.lang
```

The boxplots for both the job title groups and language preference groups seem to have minimal difference in their variance. The density plots for each group also suggests a general normality in the distribution of the groups. Given these observations, we proceeded with the needed assumptions of equal variance and normality for ANOVA and regression.

For our tests, we ran under the hypotheses:
$$
  H_0: There \ is \ no \ difference \ in \ mean \ salary \ (\mu_1=\mu_2 = ...=\mu_n)
  \\
$$
$$
  H_1: There \ is \ difference\ in \ mean \ salary
$$

```{r ref.label='Q2_ANOVA-test-general'}
```

From the ANOVA tests, when testing at the $$\alpha=0.05$$ significance level, we found the job title groups to have a p-value = `r formatC(anova.gen.jobs[[1]][["Pr(>F)"]][1],format = 'e')`, yielding strong evidence to reject the null hypothesis and implying that the mean salary of the job title groups are not equal. On the other hand, for the language groups we have the p-value = `r formatC(anova.gen.lang[[1]][["Pr(>F)"]][1],format = 'e')`, meaning we do not have strong evidence to reject the null hypothesis of equal mean salary among language preferences.

```{r, message=FALSE, echo=FALSE}
grid.table(cbind(c('Group Test','Jobs ANOVA','Lang ANOVA'),c('p-value',formatC(anova.gen.jobs[[1]][["Pr(>F)"]][1],format = 'e'),formatC(anova.gen.lang[[1]][["Pr(>F)"]][1],format = 'e'))))
```
```{r ref.label='Q2_glm-model'}
```
```{r ref.label='Q2_glmjob-assumptions'}
```
```{r ref.label='Q2_glmlang-assumptions'}
```

## Pay & Education

For our third question, we decided to look for both the overall presence of differences in salary between education levels, as well as a linear relationship between education level and salary. The original dataset contains several response categories for education level, a few of which we removed for our analyses. We removed these categories for two reasons: they have small sample sizes, and they do not represent groups we were interested in. The resulting set of education levels is:

* Professional degree
* Bachelor's degree
* Master's degree
* Doctoral degree

The smallest sample size among these groups is about 100, which is large enough to overlook the usual requirement for normal distributions in some of the following analyses.

Previous to actually running Welch T Tests or ANOVA to detect a difference between groups, we performed power caclulations on each approach. Specifically, we calculated the power to detect a difference two groups given a maximum difference of $20,000 among the groups overall (with a significance level of 5%).

```{r, echo =FALSE}
knitr::read_chunk("temp3")
```
```{r, ref.label='Q3_data'}
```
```{r, ref.label='Q3_capping'}
```
```{r, ref.label='Q3_remove_education_levels'}
```
```{r, ref.label='Q3_education_values'}
```
```{r, ref.label='Q3_Welch_power_simulation'}
```
**Power Calculations**

For the multiple Welch T Test approach, we used simulation to calculate power. We generated simulated samples for four groups from normal distributions with means ranging by $20,000, and sample sizes and variances similar to those from the groups in the real dataset. For each iteration of the simulation, a Welch T Test was run on each pair of groups. The resulting p values were compared to a significance level of 5%, reduced by Bonferroni correction. The rejections were averaged for each test and an overall rejection rate was calculated. The resulting power in our simulations was `r anyreject`.

```{r, ref.label='Q3_ANOVA_power'}
```
For ANOVA, we used an R function called *power.anova.test* to generate the power, based on the same parameters as above except that the smallest sample size was used as the size of each group. 
The resulting power was `r anova_power`.

**Testing For Difference**

On to the actual tests, we first ran the multiple Welch T Tests, one for each pairing of education levels. The resulting p values are shown below. Even with Bonferroni correction, there are three tests whose results indicate a significant difference between groups. Each test between PhD's and other groups was significant, and the rest of the tests were not. Overall, this indicates that there is significant difference in salary between some educational groups, but it further seems to suggest that the only group which is different from the rest is that of PhD's.
```{r, ref.label='Q3_Welch_tests'}
```
```{r, ref.label='Q3_ANOVA'}
```
The results of ANOVA also indicate the presence of a difference between groups, with a highly significant p value: `r anova_p`. There is a caveat to our use of ANOVA which is detailed in the "Discussion" section following this.
```{r, ref.label='Q3_education_level_ordering'}
```
The visualization below of the distributions for each group agrees with the conclusions of our tests for differences; there is a difference among groups, and PhD's exhibit the most difference.

```{r, ref.label='Q3_distributions'}
```
**Linear Regression**
```{r, ref.label='Q3_linear_regression'}
```
Beyond testing for the above, we also wanted to test for a linear relationship between education level and salary. In order to perform a regression, the education levels were considered to be ordered as follows: Professional, Bachelor's, Master's, and Doctoral. The resulting p value of `r reg_p` is evidence for a linear relationship. 

```{r, ref.label='Q3_regression_graph'}
```
The visualization above graphs the resulting linear relation along with the grouped data. This model's coefficient is `r reg_c`, leading to our conclusion that pay is higher by almost $18,000 for each level of increase in education.

## Discussion
We first address our original assumptions about Question #1: we did not ultimately receive more data from the high density areas. In fact, we received more data from the low density areas.
The data points from the dataset were indeed independent, since this survey was given to individual participants, but we did not have a normal distribution of data for commission 
or salary, nor equal variances for either of them. Subsequently, the initial metric statistics lead us to conclude that we should not use ANOVA or linear regression, as the variances are not the same. We also appreciate that unequal variances are a problem no matter how large the sample size may be, thus the Welch method is more appropriate. However, we also realize that there is a malleable window of difference (a certain thershold) that counts as equal variance, so we still performed our other tests.

Recall that commission is defined as anyone making less than $20k, all other respondents are classified as salaried.
The p-value reported from the anova tests are <2e-16 for salary and .00011 for commission. The p-value for salary and commision are 2.2e-16 and 0.0001096, respectively, so there is remarkable evidence to reject the null hypothesis. Consequently, we conclude that density does have an effect on compensation (both salary and commision). There is defintely some existing difference, and if we inlcude the results from our linear regression model, we can further point towards a possible a negative association between density and compensation.

Question 2:

Question 3:


**Limits of Analyses:**
One major limitation of the analyses presented here is that we report the results of ANOVA tests despite the fact that the groupings of data for each question did not have exactly equal variances. Though this can be a violation of the assumptions of ANOVA, we decided that the differences between variances are not very large relative to the magnitude of the variances themselves. Since there is not a simple rule for how different variances must be to invalidate ANOVA, we opted to report the results, specifically alongside the results of Welch T Tests, which are valid with the unequal variances.

limits of data too: ( I think this applies to all questions)
Question 1:

Question 2:

Question 3:
~~~ talking abt stuff in context of references ~~~

We believe our work has strong connections to workforce-oriented research in the real world. Individuals have come across conclusions that are similar to ours, albeit with varying degrees of agreement. For instance, a finance website hosts an investigative piece on the importance of education level in data sience. It deviates mildly from our findings (identifying the relationship between degree and pay as mere positive correlation), but stresses that education level explains workers' skillsets and experience -- which may then translate to career and salary growth (Hayes). Similarly, a U.S. Bureau of Labor Statistics study cofirms our suspicions that location matters a great deal -- going so far as to claim that factors like cost of living or pace of life may supercede other attributes as determinants of compensation for identical work (Torpey). 

In any case, our project has the potential to benefit not only prospective data scientists, but also to inform the industry at large as to the state of affairs in the data science arena. Our tests (and additional tests using the unexplored data in the complete survey) could continue to be used in this regard, to unearth even more surprising and beneficial patterns in this ever-growing field.  

## References

Hayes, Bob. *"When Does Education Level Matter in Data Science?"* Business Broadway, 2020. (https://businessoverbroadway.com/2016/03/14/when-does-education-level-matter-in-data-science/)

Kleiman, Iair. *"Data Scientists' Salaries Around the World V2.0."* Kaggle, 2017. (https://www.kaggle.com/kaggle/kaggle-survey-2017)

Torpey, Elka. *"Same Occupation, Different Pay: How Wages Vary."* U.S. Bureau of Labor Statistics, 2015. (https://www.bls.gov/careeroutlook/2015/article/wage-differences.htm?view_full)


