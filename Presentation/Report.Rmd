---
title: "Analyzing Compensation for Data Scientists"
author: "Anmol Srivastava, Juan Solorio, Matthew Rhodes, and Andres De La Fuente"
output: 
  pdf_document: 
classoption: twocolumn
#header-includes:
#  - \setlength{\parindent}{4em}
geometry: "left=2cm,right=2cm,top=2cm,bottom=2cm"

---

```{r setup, eval=TRUE,message=FALSE, warning=FALSE, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
library(ggplot2)
library(tidyverse)
library(ggpubr)
library(gridExtra)
library(grid)
library(scales)
```
```{r, echo =FALSE}
invisible(knitr::purl("../Code/01-Question1.Rmd", output="temp", quiet=TRUE))
```
```{r, echo =FALSE}
invisible(knitr::purl("../Code/03-Question3_AndyVersion.Rmd", output="temp3", quiet=TRUE))
```
```{r, echo =FALSE}
invisible(knitr::purl("../Code/02-Question2.Rmd", output="temp2", quiet=TRUE))
```

## Abstract
The goal of this project is to evaluate the relevance of certain factors (such as job title and education) towards observing differences in salary for working Data Scientists. The analysis employs techniques such as Welch t-tests, ANOVA and Regression, and is based on the dataset provided by Kaggle's 2017 "ML and DS Survey" (https://www.kaggle.com/kaggle/kaggle-survey-2017). The factors explored in this project are physical location (specifically whether the respondent is in a 'high density' or 'low density' area), job title, suggested programming language, and education. We found evidence (insert p-value) that physical location indicates a difference in the salary of a Data Scientist. We did not find strong evidence that programming language indicates a difference, even within individual job titles. However, we did find strong evidence for a difference in salaries between job titles. We also found evidence for a positive relation between education and salary.


## Introduction

The focus of this project is compensation within the industry of Data Science. As future employees in the field, we have an interest in which factors might relate to current Data Scientists' pay. 
To base our analysis of this topic, we decided on a dataset from Kaggle, which is the result of a wide survey conducted on people working in the field (https://www.kaggle.com/kaggle/kaggle-survey-2017). See the next section for our description of the data.

After some exploratory analysis, we came up with three questions to focus our efforts on.

**Question 1**: Do Data Scientists' salaries differ between densely populated and sparsely populated areas?

**Question 2**: Do Data Scientists' salaries differ based on their job title? Do salaries differ based on the programming languages an individual recommends?

**Question 3**: Do Data Scientists' salaries differ based on the level of education they have attained?

For each of these questions, we aimed to test for both the presence of differences between groups, and for more specific relationships by using regression.


## Data Set Description 

As referenced above, the dataset we utilized to draw our conclusions is Kaggle's 'ML and DS Survey' for the year 2017. Kaggle's description of the dataset reads as follows: "For the first time, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. The survey received over 16,000 responses and we learned a ton about who is working with data, what’s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field." The survey asked an extremely broad scope of questions, which resulted in a dataset with a mixture of multiple choice responses (e.g. "Education Level") and free form responses (e.g. "Best description of undergraduate major?").

The dataset contains the following:

*schema.csv*: a CSV file with the survey schema. This schema includes the questions that correspond to each column name in both the multipleChoiceResponses.csv and freeformResponses.csv.

*multipleChoiceResponses.csv*: Respondents' answers to multiple choice and ranking questions. These are non-randomized and thus a single row corresponds to all of a single user's answers.

*freeformResponses.csv*: Respondents' freeform answers to Kaggle's survey questions. These responses are randomized within a column, so that reading across a single row does not give a single user's answers.

*conversionRates.csv*: Currency conversion rates (to USD) as accessed from the R package "quantmod" on September 14, 2017.

*RespondentTypeREADME.txt*: This is a schema for decoding the contents of the schema.csv file. 

We used the values in the dataset's conversion rates file to generate compensation values in USD for all respondents. For the purposes of our analyses, we limited our focus to multipleChoiceReponses.csv.

Below is a simple histogram of our generated compensation data in USD from the survey before any kind of manipulation.
```{r, echo =FALSE}
knitr::read_chunk("temp3")
```
```{r ref.label='Q3_data'}
```
```{r ref.label='Q3_overall_hist'}
```
The mean for this uncleaned data is `r mn` with a maximum value of `r mx`. Further exploration of this revealed that the dataset also contains a large quantity of zeros and values that seem too low to be someone's full time pay. Based on our domain knowledge of the subject at hand, we judged that these results were unrealistic and therefore manipulated the data in a few important ways before applying our analyses to it.

First, we removed all entries with a zero for compensation, as this would either imply unemployment, entry error, or plain junk data, none of which were wanted for our analyses.

Second, we dichotomized compensation into two groups: 'salaries', and 'commissions' (the values which we decided are too low to be salaries).
We decided to split the values at 20K; anything below this is considered commission.

Third, we decided that some values on the high end were either entry error or extreme outliers (there were values ranging from millions to billions of dollars), and should be capped. The capping value was set to $500,000 based on our real world experience.

A secondary dataset was employed to help answer Question 1 (https://population.un.org/wpp/Download/Standard/Population/).

We introduced this to allow a determination of high-density and low-denisty countries. All of the countries that were included in the multipleChoiceReponses dataset were also included in the population dataset so we appended the appropriate densities to their respective rows in the multipleChoiceReponses Dataset as a final step before we started our analyses.


## Statistical Methods
**Methods:** 
The first method we thought to use was Anova.
From lecture we know that Analysis of Variance is designed to provide a single test of a null hypothesis of equal group means with a desired signiﬁcance level. It is a generalization of the equal-variance t-test to the case where the number of means to be compared is greater than 2. The second test we thought to use was linear regression.
From lecture we know that linear regression is equal to Anova when the variances are equal and the null hypotheses for ANOVA and regression are equivalent: they both imply that the mean response does not depend on the predictor.We also know that ANOVA and regression will not always agree in this way that why we decided to run them both.
After some exploratoy analysis of the data we found that the variances were not quite equal which lead us to conduct a Welch T-Test for each of the questions as well, addressing the difference in variance how this would affect the ANOVA test.

For question 1, we wanted to 
use ANOVA to test possible differences in multiple groups (groups being 1 for high and 0 for low density) and Linear Regression to test if there is a linear relationship between compensation and the density of groups.

**Assumptions:**
For question 1,
Prior to conducting our experiment some of the things that we were assuming going into this question is that the amount of information provided by the dense areas will be significantly larger than the data provided by sparse areas. We are assuming this because we think that areas that have a high population will have more of a need for data scientists than sparse areas like rural countries. Another assumption that we are making is that each one 
of the samples that are included in the dataset are independent, meaning that no answers provided by someone that took the survey affected someone else’s response. One important assumption we are making which affected the 
method we want to use is that we have a normal distribution of data.
This means that areas that are on the lower end of sparse and dense will appear as often as areas that are on the higher end of dense and sparse,
while the majority of areas fall closer to the mean of each population density. 
We initially thought that this would be a problem.Even if our data wasn’t normally distributed, since we have around 10,000 rows and roughly 4 features we think the sample size will be large enough. 
The last thing we are assuming for this is that there is Equal variance.

For question 2,


For question 3,

Explanation for why methods appropriate 


## Results 
This section contains summarized results of the analyses conducted on each of our questions.

### Question 1: Pay and Location
```{r, echo =FALSE}
knitr::read_chunk("temp")
```

```{r ref.label='Q1_data'}
```

```{r ref.label='testing'}
```
Below are histograms for commission and salary broken down into high denstiy and low density areas.
```{r ref.label='pre_plots'}
```

From Linear Regression, there is a slight negative linear relationship between density and compensation (for salary and commission).
X axis is 1 for high and 0 for low density, so we can see that as density goes from low to high on average Salary goes down by $21,652.
```{r ref.label='Question_1_plot_salary'}
```

This is the linear regression plot and the distribution of the total commission (including high and low density), similarly as density goes from low to high on average commission goes down by $2,106.
```{r ref.label='Question_1_plot_commission'}
```
```{r echo=FALSE}
unlink("temp")
```


### Question 2: Pay and Job Title / Programming Language


### Question 3: Pay and Education


## Discussion
summarize results and conclusions:
Question 1:
To adress our original assumptions about this question we did not receive more data from the high density areas we actually received more data from the low density areas.
The data points from the dataset were independent since this survey was given to individual participants, but we did not have a normal distribution of data for commission 
or for salary or equal variance for either of them.
Conclusion from the initial metric statistics, is that we should not use Anova or linear regression because the variances are not the same, and 
we know that unequal variances are a problem no matter how large the sample size thus we should use Welch. However, we also know that there
is a certain thershold that counts as equal variance so we ran the other tests anyway.
Commission was define as anyone making less than $30k and otherwise they are classified as salaried.
The p-value reported from the anova tests are <2e-16 for salary and .00011 for commission.

When looking at the p-value for salary and commision they are 2.2e-16 and 0.0001096 respectively so there is evidence to reject the null hypothesis
meaning that density has an effect on compensation for salary and commision. We know that there is defintely a difference and if we inlcude the 
results from our linear regression model, we can conclude that there is a negative association between density and compensation.

Question 2:

Question 3:


limits of analyses:
Question 1: Unequal variances.

Question 2:

Question 3:

limits of data too: ( I think this applies to all questions)
Question 1:

Question 2:

Question 3:

## References
brief 

## Appendices 
more technical aspects of analyses, any other tidbits 
